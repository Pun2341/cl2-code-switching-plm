# cl2-code-switching-plm
## Abstract

The ability of language models to capture code-switching (CS) is crucial for processing real-world data from multilingual speakers. In this paper, we examine the intersection between multilingual pre-trained language models (PLMs) and code-switching through replicating two CS detection tasks: sentence classification and token-level language identification, and our results confirm previous findings on these tasks. Following this, we explore the effect of fine-tuning multilingual BERT on sentiment analysis and part-of-speech tagging using datasets of different languages. We found that the bilingual model performed best overall, which the English and CS-trained models performed best on CS data. We further find that accuracy for POS tagging is high overall, especially for the model fine-tuned on bilingual data, but CS test data is best captured by a model fine-tuned on CS data. This highlights the need to consider CS-specific tasks in development of multilingual models.
